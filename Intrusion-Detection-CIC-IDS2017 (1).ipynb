{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqiP1ftFazMG"
   },
   "source": [
    "# **Analysis of the Intrusion Detection Evaluation Dataset (CIC-IDS2017)**\n",
    "\n",
    "[Dataset Link](https://www.unb.ca/cic/datasets/ids-2017.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxU8-WULcl3P"
   },
   "source": [
    "## 1. An overview of the Dataset including the Dataset Characteristics and Exploratory Data Analysis, Data Preprocessing, and performance of different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJU7_XXQeRCW"
   },
   "source": [
    "- **Dataset Characteristics:** CIC-IDS2017 dataset contains network traffic data for the development and evaluation of intrusion detection systems. The dataset is designed to be representative of modern network traffic and includes more than 2.8 million network packets captured over a period of seven days in a real network environment. The dataset includes normal traffic and seven different attack scenarios: Brute Force, Heartbleed, Botnet, DoS, DDoS, Web Attack and Infiltration. The dataset is highly imbalanced. The majority of records belongs to the 'Benign' class and relatively few records belongs to the other classes. The dataset consists of 2830743 rows and 79 columns. In these columns, 78 of them are features that are numerical and the 'label' column is categorical.\n",
    "\n",
    "- **Exploratory Data Analysis:** This dataset have many duplicate values (308381), which creates bias that is not good for the machine learning model. The number of missing values and infinite values on two columns 'Flow Bytes/s' and 'Flow Packets/s' are very few comparing to the size of the dataset which is only 0.06% (1564). However, the values were handled using appropriate methods.\n",
    "\n",
    "- **Data Preprocessing:** We dropped the duplicates, replaced any infinite values (positive or negative) with NaN (not a number) and filled the missing values with median values. Since we have a very large dataset, the initial memory usage is quite high leading to session crashes. We later worked our way through this by down casting data types based on the min and max values available. We try to reduce the memory usages that is helpful for our model.\n",
    "\n",
    "- **Data Analysis:** We grouped similar attacks together to analyze the dataset and identify patterns in the different types of attacks. We took a sample from the population (20%). Later we did some data analysis which consist of plotting various kinds of charts, correlation matrices etc. to see the relationships between features, types of attacks present in the dataset etc. In our analysis, we noticed there are a good number of features that are strongly, even directly correlated with other features (both positive and negative). This is an issue it introduces multi-collinearity which can highly impact the machine learning models that we will develop later.\n",
    "\n",
    "  Also, as the dataset is quite huge and has more than 70 features, it would be extremely difficult to train models using limited resources. In order to overcome this issue, we used PCA (Principal Component Analysis) to reduece dimensions. We played with the number of components to see how much information we can preserve. We monitored the '*explained_variance_ratio_*' to make sure we retain most informations. However, it was a bit challenging to reduce dimensions while preserving the information to train the models. We performed StandardScaler before performing Incremental PCA.\n",
    "\n",
    "- **ML Models:** It is worth mentioning the fact that the following dataset is highly imbalanced. So, we created a balanced dataset out of this with our domain knowledge to train various ML models. Since our dataset is quite large and has a reasonable amount of samples to train and test different ML models using various classification algorithms (Logistic Regression, Support Vector Machine for Binary Classification and Random Forest Classifier, Decision Tree, K Nearest Neighbours for Multi-class Classification).\n",
    "\n",
    "  For binary classifications, we trained the models to distinguish between normal traffic and anomalous traffic. This means it will only predict whether an intrusion is taking place or not. Alternatively, using the multi-class classification algorithms, we further extended our prediction capabilities to identify which type of attack or intrusion is taking place. We tried both binary classifications and multi-class classifications to see how the data holds up. Later we cross-validated, evaluated and compared those models to see which one works better or worse.\n",
    "\n",
    "- **Performance Evaluation:** After training multiple machine learning models, we proceeded to evaluate their respective performances. Our evaluation process involved comparing the accuracy, recall, f1-score and confusion matrix of each model. Through analysis of the results, we were able to see which model performed the best and which performed worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPwNIskNhEHI"
   },
   "source": [
    "## 2.\tDataset Characteristics and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8DGk4gEhiJp",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.1 Load, View Data and Show Analysis on Rows and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wv4QVWUBxwu-",
    "outputId": "2a68bbee-bd0d-4b5c-f59e-96af8158a183"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZZzVucCEfoi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "sns.set(style='darkgrid')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-Ma89nNhldR"
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "data1 = pd.read_csv('/content/drive/MyDrive/MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv')\n",
    "data2 = pd.read_csv('/content/drive/MyDrive/MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv')\n",
    "data3 = pd.read_csv('/content/drive/MyDrive/MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv')\n",
    "data4 = pd.read_csv('/content/drive/MyDrive/MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv')\n",
    "data5 = pd.read_csv('/content/drive/MyDrive/MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv')\n",
    "data6 = pd.read_csv('/content/drive/MyDrive/MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv')\n",
    "data7 = pd.read_csv('/content/drive/MyDrive/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv')\n",
    "data8 = pd.read_csv('/content/drive/MyDrive/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EuzF1FT5IEyr",
    "outputId": "c5f5b9db-6091-4f8a-d8ff-d3b48f8d510a"
   },
   "outputs": [],
   "source": [
    "data_list = [data1, data2, data3, data4, data5, data6, data7, data8]\n",
    "\n",
    "print('Data dimensions: ')\n",
    "for i, data in enumerate(data_list, start = 1):\n",
    "  rows, cols = data.shape\n",
    "  print(f'Data{i} -> {rows} rows, {cols} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47AUI4Q9GiBt",
    "outputId": "105406bc-2fb0-4b5f-b3e4-98acacb78e2d"
   },
   "outputs": [],
   "source": [
    "data = pd.concat(data_list)\n",
    "rows, cols = data.shape\n",
    "\n",
    "print('New dimension:')\n",
    "print(f'Number of rows: {rows}')\n",
    "print(f'Number of columns: {cols}')\n",
    "print(f'Total cells: {rows * cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3VWo9AZa0CP"
   },
   "outputs": [],
   "source": [
    "# Deleting dataframes after concating to save memory\n",
    "for d in data_list: del d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKja3Q5r3AiS"
   },
   "outputs": [],
   "source": [
    "# Renaming the columns by removing leading/trailing whitespace\n",
    "col_names = {col: col.strip() for col in data.columns}\n",
    "data.rename(columns = col_names, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydfEc-jBstvg",
    "outputId": "059ca16f-5707-429a-cb05-d896d853c132"
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4w14PfaRt9ZH",
    "outputId": "2d2e74ee-8964-441e-c9cc-7ead85214b67"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Tt8K5je1auwT",
    "outputId": "35cc864d-3162-4b44-fde3-b6529407a5e8"
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 80\n",
    "\n",
    "print('Overview of Columns:')\n",
    "data.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "y4UHpc39vmst",
    "outputId": "4c86e5e6-5ed1-4675-97c5-6f14850b86fa"
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 80\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSAAWG06ekQn"
   },
   "source": [
    "### 2.2 Data Cleaning\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDdJgFihgVf8"
   },
   "source": [
    "#### Identifying duplicate values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eCLMpzvegtHo",
    "outputId": "82144560-876c-447b-f9bb-0d4e5a2eb0e5"
   },
   "outputs": [],
   "source": [
    "dups = data[data.duplicated()]\n",
    "print(f'Number of duplicates: {len(dups)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZRWCnHg-Rfh",
    "outputId": "0686cdde-07be-4add-dd43-fe664b7733f2"
   },
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R28LNrp5iWR7"
   },
   "source": [
    "#### Identifying missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIwPAME64Xvd",
    "outputId": "a7f80264-e534-41e7-f2c9-574b1f441863"
   },
   "outputs": [],
   "source": [
    "missing_val = data.isna().sum()\n",
    "print(missing_val.loc[missing_val > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w84MJW5o45KG",
    "outputId": "f5975086-f911-4747-89f1-58f8b27a84aa"
   },
   "outputs": [],
   "source": [
    "# Checking for infinity values\n",
    "numeric_cols = data.select_dtypes(include = np.number).columns\n",
    "inf_count = np.isinf(data[numeric_cols]).sum()\n",
    "print(inf_count[inf_count > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIsiR9JC1ccV",
    "outputId": "896135f3-ef18-450a-ca0a-f0e1d3fb62c7"
   },
   "outputs": [],
   "source": [
    "# Replacing any infinite values (positive or negative) with NaN (not a number)\n",
    "print(f'Initial missing values: {data.isna().sum().sum()}')\n",
    "\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "\n",
    "print(f'Missing values after processing infinite values: {data.isna().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SYdMMTNf3lqD",
    "outputId": "9c0ca587-c913-444f-abd2-73cb53af83d6"
   },
   "outputs": [],
   "source": [
    "missing = data.isna().sum()\n",
    "print(missing.loc[missing > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-sPt__O_sjCd",
    "outputId": "7e48e48b-78da-45e2-e18c-886626af5760"
   },
   "outputs": [],
   "source": [
    "# Calculating missing value percentage in the dataset\n",
    "mis_per = (missing / len(data)) * 100\n",
    "mis_table = pd.concat([missing, mis_per.round(2)], axis = 1)\n",
    "mis_table = mis_table.rename(columns = {0 : 'Missing Values', 1 : 'Percentage of Total Values'})\n",
    "\n",
    "print(mis_table.loc[mis_per > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xf9z8Pwlid-3"
   },
   "source": [
    "#### Visualisation of missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "DibPfnKeXAWS",
    "outputId": "ed29aa4d-4496-46e1-f66c-9a7fd861b7a2"
   },
   "outputs": [],
   "source": [
    "sns.set_palette('pastel')\n",
    "colors = sns.color_palette()\n",
    "\n",
    "missing_vals = [col for col in data.columns if data[col].isna().any()]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (2, 6))\n",
    "msno.bar(data[missing_vals], ax = ax, fontsize = 12, color = colors)\n",
    "ax.set_xlabel('Features', fontsize = 12)\n",
    "ax.set_ylabel('Non-Null Value Count', fontsize = 12)\n",
    "ax.set_title('Missing Value Chart', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXWOYP6-ieI0"
   },
   "source": [
    "#### Dealing with missing values (Columns with missing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "E_wW5arHZUo5",
    "outputId": "50e328dc-a2b8-44a0-fe00-3a2c664daf36"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 3))\n",
    "sns.boxplot(x = data['Flow Bytes/s'])\n",
    "plt.xlabel('Boxplot of Flow Bytes/s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "vqQzdH7ccwv0",
    "outputId": "6ab7b050-df82-44e4-8529-7ae675baf01b"
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette('Blues')\n",
    "plt.hist(data['Flow Bytes/s'], color = colors[1])\n",
    "plt.title('Histogram of Flow Bytes/s')\n",
    "plt.xlabel('Flow Bytes/s')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "kjaJVgULdCZx",
    "outputId": "1aaa647d-b19d-4db6-b538-8c17e911f5b1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 3))\n",
    "sns.boxplot(x = data['Flow Packets/s'])\n",
    "plt.xlabel('Boxplot of Flow Packets/s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "KVYQ07HvdOby",
    "outputId": "4396ca68-3ecf-4831-fb45-7cdc080648a4"
   },
   "outputs": [],
   "source": [
    "plt.hist(data['Flow Packets/s'], color = colors[1])\n",
    "plt.title('Histogram of Flow Packets/s')\n",
    "plt.xlabel('Flow Packets/s')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZMitp-BBZvl8",
    "outputId": "118f84b0-410c-4a16-a2ac-acc297a32e92"
   },
   "outputs": [],
   "source": [
    "med_flow_bytes = data['Flow Bytes/s'].median()\n",
    "med_flow_packets = data['Flow Packets/s'].median()\n",
    "\n",
    "print('Median of Flow Bytes/s: ', med_flow_bytes)\n",
    "print('Median of Flow Packets/s: ', med_flow_packets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4_mBQswOvNE"
   },
   "outputs": [],
   "source": [
    "# Filling missing values with median\n",
    "data['Flow Bytes/s'].fillna(med_flow_bytes, inplace = True)\n",
    "data['Flow Packets/s'].fillna(med_flow_packets, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ryxi5CZzayJq",
    "outputId": "b9ddfc62-4040-4580-9d40-77e7dca58562"
   },
   "outputs": [],
   "source": [
    "print('Number of \\'Flow Bytes/s\\' missing values:', data['Flow Bytes/s'].isna().sum())\n",
    "print('Number of \\'Flow Packets/s\\' missing values:', data['Flow Packets/s'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rIU56yNiPxq"
   },
   "source": [
    "- The first step is to identify duplicate rows and missing or invalid values. We\n",
    "identified and dropped the duplicate rows (308381 rows). From the data description, we identified that the dataset has infinity values. So, we checked and replaced the positive or negative infinity values with NaN (not a number) and counted it as a missing value. In the dataset, two features, FlowBytes/s, and Flow Packets/s contain missing values. For both columns, the number of missing values is 1564 which is 0.06% of total values.\n",
    "\n",
    "- Flow Bytes/s and Flow Packets/s are continuous variables. We can see from the Flow Bytes/s and Flow Packets/s histogram and box plot that the majority of values are towards one area which indicates that the data is not normally distributed. The box plot of the Flow Bytes/s and Flow Packets/s shows that the variables have extreme values or outliers. So, our strategy is to fill in missing values with median value. Because, filling the missing values with the median does not introduce any new categories or disrupt the distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsSwQ-DOvm-6"
   },
   "source": [
    "### 2.3 Analysing Patterns using Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGkxFBOYv5Gt"
   },
   "source": [
    "#### Visualization of column correlation. Also, plotting Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6no9oLuBvnST",
    "outputId": "ae75dd0c-010d-4383-e21a-95fc1f775d01"
   },
   "outputs": [],
   "source": [
    "data['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c-cQD00vuL5",
    "outputId": "2417238c-b9cd-4685-f677-680eec2ff636"
   },
   "outputs": [],
   "source": [
    "# Types of attacks & normal instances (BENIGN)\n",
    "data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lt1zWoqUwKa0"
   },
   "outputs": [],
   "source": [
    "# Creating a dictionary that maps each label to its attack type\n",
    "attack_map = {\n",
    "    'BENIGN': 'BENIGN',\n",
    "    'DDoS': 'DDoS',\n",
    "    'DoS Hulk': 'DoS',\n",
    "    'DoS GoldenEye': 'DoS',\n",
    "    'DoS slowloris': 'DoS',\n",
    "    'DoS Slowhttptest': 'DoS',\n",
    "    'PortScan': 'Port Scan',\n",
    "    'FTP-Patator': 'Brute Force',\n",
    "    'SSH-Patator': 'Brute Force',\n",
    "    'Bot': 'Bot',\n",
    "    'Web Attack � Brute Force': 'Web Attack',\n",
    "    'Web Attack � XSS': 'Web Attack',\n",
    "    'Web Attack � Sql Injection': 'Web Attack',\n",
    "    'Infiltration': 'Infiltration',\n",
    "    'Heartbleed': 'Heartbleed'\n",
    "}\n",
    "\n",
    "# Creating a new column 'Attack Type' in the DataFrame based on the attack_map dictionary\n",
    "data['Attack Type'] = data['Label'].map(attack_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SWLCTLRyxxW0",
    "outputId": "98919350-b090-45f2-cd23-b8ab201e52bc"
   },
   "outputs": [],
   "source": [
    "data['Attack Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4tyKwUlwTPw"
   },
   "outputs": [],
   "source": [
    "data.drop('Label', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fId7C3nUyNZ1",
    "outputId": "daf54ba2-fcd0-4947-ba0f-29fd4a15c6ec"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['Attack Number'] = le.fit_transform(data['Attack Type'])\n",
    "\n",
    "print(data['Attack Number'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJhWdou_yN9-",
    "outputId": "6f096b16-3ee8-492e-c068-8d2eec5f24f0"
   },
   "outputs": [],
   "source": [
    "# Printing corresponding attack type for each encoded value\n",
    "encoded_values = data['Attack Number'].unique()\n",
    "for val in sorted(encoded_values):\n",
    "    print(f\"{val}: {le.inverse_transform([val])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "_O1N0PzUyOIq",
    "outputId": "ffdb9756-324b-4a50-eacd-2f73b15647c4"
   },
   "outputs": [],
   "source": [
    "corr = data.corr(numeric_only = True).round(2)\n",
    "corr.style.background_gradient(cmap = 'coolwarm', axis = None).format(precision = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "eejPjRDA1OBq",
    "outputId": "e64a1935-3898-49b1-873f-c09e368990fd"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (24, 24))\n",
    "sns.heatmap(corr, cmap = 'coolwarm', annot = False, linewidth = 0.5)\n",
    "plt.title('Correlation Matrix', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0xq3ZryyOUL",
    "outputId": "a98c358f-2f09-43a9-ef03-a60453ee4c02"
   },
   "outputs": [],
   "source": [
    "# Positive correlation features for 'Attack Number'\n",
    "pos_corr_features = corr['Attack Number'][(corr['Attack Number'] > 0) & (corr['Attack Number'] < 1)].index.tolist()\n",
    "\n",
    "print(\"Features with positive correlation with 'Attack Number':\\n\")\n",
    "for i, feature in enumerate(pos_corr_features, start = 1):\n",
    "    corr_value = corr.loc[feature, 'Attack Number']\n",
    "    print('{:<3} {:<24} :{}'.format(f'{i}.', feature, corr_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_ahQcNdyOgI",
    "outputId": "4a6021a1-1268-45e7-83e6-75b229e616fc"
   },
   "outputs": [],
   "source": [
    "print(f'Number of considerable important features: {len(pos_corr_features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUxccTMBPvdJ",
    "outputId": "6384f31c-3195-4bcc-c88c-24843cef363b"
   },
   "outputs": [],
   "source": [
    "# Checking for columns with zero standard deviation (the blank squares in the heatmap)\n",
    "std = data.std(numeric_only = True)\n",
    "zero_std_cols = std[std == 0].index.tolist()\n",
    "zero_std_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqV63kz3OesE"
   },
   "source": [
    "- We mapped each label to the corresponding attack type. This groups similar attacks together and provides an easier and more interpretable way to analyze the dataset and identify patterns in the different types of attacks.\n",
    "\n",
    "- For plotting the correlation matrix, we encoded the 'Attack Type' column and plotted the heatmap. From the heatmap, we observe that there are many pairs of highly correlated features. Highly correlated features in the dataset are problematic and lead to overfitting. A positive correlation exists when one variable decreases as the other variable decreases or one variable increases while the other increases. There are 32 features with positive correlations that may help in predicting the target feature.\n",
    "\n",
    "- The columns with zero standard deviation have the same value in all rows. These columns don't have any variance. It simply means that there is no meaningful relationship with any other columns which results in NaN correlation cofficient. These columns cannot help differentiate between the classes or groups of data. So, these zero standard deviation columns don't contribute to the correlation matrix and will appear blank in the heatmap. This can be helpful while doing data processing as we may drop the columns if we find out that these columns has no variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uDiDKokwPhS"
   },
   "source": [
    "#### Visualization of Linear Relationships of columns (Continuous Numerical Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1kg3zyroTrKO",
    "outputId": "0f706a60-66a4-428c-9fc7-99cf5a66127f"
   },
   "outputs": [],
   "source": [
    "# Data sampling for data analysis\n",
    "sample_size = int(0.2 * len(data)) # 20% of the original size\n",
    "sampled_data = data.sample(n = sample_size, replace = False, random_state = 0)\n",
    "sampled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HMglBEJeT5Xk",
    "outputId": "445de183-196b-4615-e6bc-871aa2669c82"
   },
   "outputs": [],
   "source": [
    "# To assess if a sample is representative of the population and comparison of descriptive statistics (mean)\n",
    "numeric_cols = data.select_dtypes(include = [np.number]).columns.tolist()\n",
    "print('Descriptive Statistics Comparison (mean):\\n')\n",
    "print('{:<32s}{:<22s}{:<22s}{}'.format('Feature', 'Original Dataset', 'Sampled Dataset', 'Variation Percentage'))\n",
    "print('-' * 96)\n",
    "\n",
    "high_variations = []\n",
    "for col in numeric_cols:\n",
    "    old = data[col].describe()[1]\n",
    "    new = sampled_data[col].describe()[1]\n",
    "    if old == 0:\n",
    "        pct = 0\n",
    "    else:\n",
    "        pct = abs((new - old) / old)\n",
    "    if pct * 100 > 5:\n",
    "        high_variations.append((col, pct * 100))\n",
    "    print('{:<32s}{:<22.6f}{:<22.6f}{:<2.2%}'.format(col, old, new, pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "id": "vFX4uTA5UflZ",
    "outputId": "eafd0dbe-3516-4715-abc3-c68e7c8b2268"
   },
   "outputs": [],
   "source": [
    "labels = [t[0] for t in high_variations]\n",
    "values = [t[1] for t in high_variations]\n",
    "\n",
    "colors = sns.color_palette('Blues', n_colors=len(labels))\n",
    "fig, ax = plt.subplots(figsize = (10, 5))\n",
    "ax.bar(labels, values, color = colors)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    ax.text(i, values[i], str(round(values[i], 2)), ha = 'center', va = 'bottom', fontsize = 10)\n",
    "\n",
    "plt.xticks(rotation = 90)\n",
    "ax.set_title('Variation percenatge of the features of the sample which\\n mean value variates higher than 5% of the actual mean')\n",
    "ax.set_ylabel('Percentage (%)')\n",
    "ax.set_yticks(np.arange(0, 41, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FaTNH4jv0CTq",
    "outputId": "392895a3-b854-4640-8141-b60e39655e05"
   },
   "outputs": [],
   "source": [
    "# Printing the unique value count\n",
    "indent = '{:<3} {:<30}: {}'\n",
    "print('Unique value count for: ')\n",
    "for i, feature in enumerate(list(sampled_data.columns)[:-1], start = 1):\n",
    "    print(indent.format(f'{i}.', feature, sampled_data[feature].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "g4ujCskP0Hbf",
    "outputId": "b3dbd7f2-191b-4a68-a6a6-84aa3b5532fd"
   },
   "outputs": [],
   "source": [
    "'''Generating a set of visualizations for columns that have more than one unique value but less than 50 unique values.\n",
    "For categorical columns, a bar plot is generated showing the count of each unique value.\n",
    "For numerical columns, a histogram is generated.'''\n",
    "unique_values = sampled_data.nunique()\n",
    "selected_cols = sampled_data[[col for col in sampled_data if 1 < unique_values[col] < 50]]\n",
    "rows, cols = selected_cols.shape\n",
    "col_names = list(selected_cols)\n",
    "num_of_rows = (cols + 3) // 4\n",
    "\n",
    "color_palette = sns.color_palette('Blues', n_colors = 3)\n",
    "plt.figure(figsize = (6 * 4, 8 * num_of_rows))\n",
    "\n",
    "for i in range(cols):\n",
    "    plt.subplot(num_of_rows, 4, i + 1)\n",
    "    col_data = selected_cols.iloc[:, i]\n",
    "    if col_data.dtype.name == 'object':\n",
    "        col_data.value_counts().plot(kind = 'bar', color = color_palette[2])\n",
    "    else:\n",
    "        col_data.hist(color = color_palette[0])\n",
    "\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.title(col_names[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_Lj5L_2RUvTf",
    "outputId": "24406221-4358-4b28-dc47-625d5b619fc4"
   },
   "outputs": [],
   "source": [
    "# Correlation matrix for sampled data\n",
    "corr_matrix = sampled_data.corr(numeric_only = True).round(2)\n",
    "corr_matrix.style.background_gradient(cmap = 'coolwarm', axis = None).format(precision = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "16eLCj4YVFw-",
    "outputId": "58b7e8c4-8a39-47a1-908e-1ef86c733a1b"
   },
   "outputs": [],
   "source": [
    "# Plotting the pairs of strongly positive correlated features in the sampled_data that have a correlation coefficient of 0.85 or higher\n",
    "cols = list(sampled_data.columns)[:-2]\n",
    "high_corr_pairs = []\n",
    "corr_th = 0.85\n",
    "\n",
    "for i in range(len(cols)):\n",
    "  for j in range(i + 1, len(cols)):\n",
    "    val = sampled_data[cols[i]].corr(sampled_data[cols[j]])\n",
    "    # If the correlation coefficient is NaN or below the threshold, skip to the next pair\n",
    "    if np.isnan(val) or val < corr_th:\n",
    "      continue\n",
    "    high_corr_pairs.append((val, cols[i], cols[j]))\n",
    "\n",
    "size, cols = len(high_corr_pairs), 4\n",
    "rows, rem =  size // cols, size % cols\n",
    "if rem:\n",
    "  rows += 1\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize = (24, int(size * 1.7)))\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "      try:\n",
    "        val, x, y = high_corr_pairs[i * cols + j]\n",
    "        if val > 0.99:\n",
    "          axs[i, j].scatter(sampled_data[x], sampled_data[y], color = 'green', alpha = 0.1)\n",
    "        else:\n",
    "          axs[i, j].scatter(sampled_data[x], sampled_data[y], color = 'blue', alpha = 0.1)\n",
    "        axs[i, j].set_xlabel(x)\n",
    "        axs[i, j].set_ylabel(y)\n",
    "        axs[i, j].set_title(f'{x} vs\\n{y} ({val:.2f})')\n",
    "      except IndexError:\n",
    "        fig.delaxes(axs[i, j])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkXVvSrux4i8"
   },
   "outputs": [],
   "source": [
    "sampled_data.drop('Attack Number', axis = 1, inplace = True)\n",
    "data.drop('Attack Number', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mRehOiWZuvvF",
    "outputId": "8a5b6661-189b-41c7-dd21-d93f9c006c65"
   },
   "outputs": [],
   "source": [
    "# Identifying outliers\n",
    "numeric_data = sampled_data.select_dtypes(include = ['float', 'int'])\n",
    "q1 = numeric_data.quantile(0.25)\n",
    "q3 = numeric_data.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "outlier = (numeric_data < (q1 - 1.5 * iqr)) | (numeric_data > (q3 + 1.5 * iqr))\n",
    "outlier_count = outlier.sum()\n",
    "outlier_percentage = round(outlier.mean() * 100, 2)\n",
    "outlier_stats = pd.concat([outlier_count, outlier_percentage], axis = 1)\n",
    "outlier_stats.columns = ['Outlier Count', 'Outlier Percentage']\n",
    "\n",
    "print(outlier_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xtY6mgJuxqt",
    "outputId": "cae39d72-fa71-4f0f-ee21-c75d6bf0c4c9"
   },
   "outputs": [],
   "source": [
    "# Identifying outliers based on attack type\n",
    "outlier_counts = {}\n",
    "for i in numeric_data:\n",
    "    for attack_type in sampled_data['Attack Type'].unique():\n",
    "        attack_data = sampled_data[i][sampled_data['Attack Type'] == attack_type]\n",
    "        q1, q3 = np.percentile(attack_data, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        num_outliers = ((attack_data < lower_bound) | (attack_data > upper_bound)).sum()\n",
    "        outlier_percent = num_outliers / len(attack_data) * 100\n",
    "        outlier_counts[(i, attack_type)] = (num_outliers, outlier_percent)\n",
    "\n",
    "for i in numeric_data:\n",
    "  print(f'Feature: {i}')\n",
    "  for attack_type in sampled_data['Attack Type'].unique():\n",
    "    num_outliers, outlier_percent = outlier_counts[(i, attack_type)]\n",
    "    print(f'- {attack_type}: {num_outliers} ({outlier_percent:.2f}%)')\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787
    },
    "id": "tUUeIrRSvI73",
    "outputId": "097a8369-463e-4a0b-bbd9-53a35186bc5d"
   },
   "outputs": [],
   "source": [
    "# Plotting the percentage of outliers that are higher than 20%\n",
    "fig, ax = plt.subplots(figsize = (24, 10))\n",
    "for i in numeric_data:\n",
    "    for attack_type in sampled_data['Attack Type'].unique():\n",
    "        num_outliers, outlier_percent = outlier_counts[(i, attack_type)]\n",
    "        if outlier_percent > 20:\n",
    "            ax.bar(f'{i} - {attack_type}', outlier_percent)\n",
    "\n",
    "ax.set_xlabel('Feature-Attack Type')\n",
    "ax.set_ylabel('Percentage of Outliers')\n",
    "ax.set_title('Outlier Analysis')\n",
    "ax.set_yticks(np.arange(0, 41, 10))\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tlRhXeZoze8"
   },
   "source": [
    "- As we have a large dataset, it was both time-consuming and computationally expensive to do all the analysis on the original-sized dataset. Therefore, we sampled 20% of the dataset to do our computationally expensive analysis. We also assessed whether the sample is representative of the population by doing a comparison of descriptive statistics (mean) and features that variates 5%\n",
    "higher than the actual mean values of the dataset.\n",
    "\n",
    "- A histogram for numerical columns and a bar plot for categorical columns are generated that have more than one unique value and less than 50 unique values. The plots visualize the distribution of data in a quick and easier way. This visualizes patterns like the distribution of values in numerical columns and common categories in categorical columns. It is used to understand the relationships between different variables and identify anomalies in the data.\n",
    "\n",
    "- The scatter plots show the relationship between strongly positive correlated features with a correlation coefficient of 0.85 or higher. Blue scatter plot points show the correlation coefficient pairs less than 0.99 and green scatter plot points show the pairs with 0.99 or almost 1.0. From these plots, we can visualize linear relationships between the features or identify indications of multicollinearity between features where two or more predictors are highly correlated. Highly correlated features introduce multicollinearity which causes problems for machine learning algorithms because it assumes that the features are independent. From some of the plots, we can see that there is a tight cluster of data points around\n",
    "the straight line where the correlation coefficient is close to 1.\n",
    "\n",
    "- We identified the outliers of each feature based on attack types and found that this dataset contains many outliers. Outliers increase variability in the dataset. But in the dataset, outliers may indicate different patterns like network intrusion attempts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iYfyW2-yyAR"
   },
   "source": [
    "#### Visualization of column relationships (Categorical Variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrzKgheVy967"
   },
   "source": [
    "All the features in our dataset is numerical. We have one Categorical Variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "gVvkohfgBMCT",
    "outputId": "4f875e7d-93fa-4a72-cd23-fe1cf3f08bca"
   },
   "outputs": [],
   "source": [
    "# Different 'Attack Type' in the main dataset excluding 'BENIGN'\n",
    "attacks = data.loc[data['Attack Type'] != 'BENIGN']\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "ax = sns.countplot(x = 'Attack Type', data = attacks, palette = 'pastel', order = attacks['Attack Type'].value_counts().index)\n",
    "plt.title('Types of attacks')\n",
    "plt.xlabel('Attack Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation = 90)\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2, p.get_height() + 1000), ha = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "_PQc5IIUBM0o",
    "outputId": "92a36639-36f4-45c4-d4a6-303e86128ba1"
   },
   "outputs": [],
   "source": [
    "attack_counts = attacks['Attack Type'].value_counts()\n",
    "threshold = 0.005\n",
    "percentages = attack_counts / attack_counts.sum()\n",
    "small_slices = percentages[percentages < threshold].index.tolist()\n",
    "attack_counts['Other'] = attack_counts[small_slices].sum()\n",
    "attack_counts.drop(small_slices, inplace = True)\n",
    "\n",
    "sns.set_palette('pastel')\n",
    "plt.figure(figsize = (8, 8))\n",
    "plt.pie(attack_counts.values, labels = attack_counts.index, autopct = '%1.1f%%', textprops={'fontsize': 6})\n",
    "plt.title('Distribution of Attack Types')\n",
    "plt.legend(attack_counts.index, loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Pxl8l92KBn2Y",
    "outputId": "b1c0d5d7-a759-49bc-8ccc-48c63a4a1784"
   },
   "outputs": [],
   "source": [
    "# Creating a boxplot for each attack type with the columns of sampled dataset\n",
    "for attack_type in sampled_data['Attack Type'].unique():\n",
    "    attack_data = sampled_data[sampled_data['Attack Type'] == attack_type]\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.boxplot(data = attack_data.drop(columns = ['Attack Type']), orient = 'h')\n",
    "    plt.title(f'Boxplot of Features for Attack Type: {attack_type}')\n",
    "    plt.xlabel('Feature Value')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "ZRhokdjOBU1F",
    "outputId": "49cf3e10-cf3f-4c3b-c9cf-6bd27efb0daf"
   },
   "outputs": [],
   "source": [
    "data.groupby('Attack Type').first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJ9tbZTRxHVU"
   },
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNRw8GcEQ-st"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dAqW7PSTnsf-",
    "outputId": "fceb4cd3-0a18-4dfc-b83b-3ef55de730e0"
   },
   "outputs": [],
   "source": [
    "# For improving performance and reduce memory-related errors\n",
    "old_memory_usage = data.memory_usage().sum() / 1024 ** 2\n",
    "print(f'Initial memory usage: {old_memory_usage:.2f} MB')\n",
    "for col in data.columns:\n",
    "    col_type = data[col].dtype\n",
    "    if col_type != object:\n",
    "        c_min = data[col].min()\n",
    "        c_max = data[col].max()\n",
    "        # Downcasting float64 to float32\n",
    "        if str(col_type).find('float') >= 0 and c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "            data[col] = data[col].astype(np.float32)\n",
    "\n",
    "        # Downcasting int64 to int32\n",
    "        elif str(col_type).find('int') >= 0 and c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "            data[col] = data[col].astype(np.int32)\n",
    "\n",
    "new_memory_usage = data.memory_usage().sum() / 1024 ** 2\n",
    "print(f\"Final memory usage: {new_memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ncQeTyOOpl1N",
    "outputId": "4f1136d4-55fe-4f85-f6f3-31b10d1bc236"
   },
   "outputs": [],
   "source": [
    "# Calculating percentage reduction in memory usage\n",
    "print(f'Reduced memory usage: {1 - (new_memory_usage / old_memory_usage):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YCZ4UmTaB_I7",
    "outputId": "5c1bce54-bba4-4a72-b07b-8b3a974190d2"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vFidCf0oqvii",
    "outputId": "cbf2228b-484f-4c7e-e43b-781afa3870b6"
   },
   "outputs": [],
   "source": [
    "data.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z5AkJSL0kGX2",
    "outputId": "7c5f500a-4769-493b-c362-2c18872e30aa"
   },
   "outputs": [],
   "source": [
    "# Dropping columns with only one unique value\n",
    "num_unique = data.nunique()\n",
    "one_variable = num_unique[num_unique == 1]\n",
    "not_one_variable = num_unique[num_unique > 1].index\n",
    "\n",
    "dropped_cols = one_variable.index\n",
    "data = data[not_one_variable]\n",
    "\n",
    "print('Dropped columns:')\n",
    "dropped_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UA8wz7Y0kj0g",
    "outputId": "1edcb84e-0212-4051-f446-bdb6789e008a"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7R4OKANLsibW",
    "outputId": "c4cb0c23-8ccb-49f3-8a2f-6f44c030e9c6"
   },
   "outputs": [],
   "source": [
    "# Columns after removing non variant columns\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "482NezsW-1Ua"
   },
   "source": [
    "- To improve performance and reduce the risk of memory-related errors (mostly session crashes), we downcasted the float and integer values based on the presence of the minimum and maximum values and reduced memory usage by 47.5%.\n",
    "\n",
    "- The columns with zero standard deviation have the same value in all rows.\n",
    "These columns don't have any variance. It simply means that there is no meaningful relationship with any other columns. These columns cannot help differentiate between the classes or groups of data. So, we dropped the columns that have no variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kk2Kas03g1V3"
   },
   "source": [
    "### Applying PCA to reduce dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZfQoEn1LO49"
   },
   "source": [
    "A simple and effective way to reduce the dimensionality of the dataset and improve the performance of the model is to use strongly correlated features. We used label encoding on the target feature where the numerical values assigned to each category do not have inherent meaning and they are arbitrary. For this reason, the correlation matrix calculated using label-encoded variables may not accurately reflect the true relationships between the variables.\n",
    "\n",
    "So, a more flexible approach to feature selection can be PCA. PCA is a technique that transforms original set of variables into a smaller set of uncorrelated variables, called principal components.\n",
    "\n",
    "PCA can capture more complex relationships between variables that may not be evident from correlation matrix analysis. It can also help to reduce the risk of overfitting.\n",
    "\n",
    "Here, we applied Incremental PCA. Incremental PCA is a variant of PCA that allows for the efficient computation of principal components of a large dataset that cannot be stored in memory.\n",
    "\n",
    "We applied StandardScaler before performing Incremental PCA to standardize the data values into a standard format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4tIrp4FCiw0"
   },
   "outputs": [],
   "source": [
    "# Standardizing the dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features = data.drop('Attack Type', axis = 1)\n",
    "attacks = data['Attack Type']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ADsMJMUgI-z",
    "outputId": "09ed3ea9-cc30-4e51-b110-fc440a681854"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "size = len(features.columns) // 2\n",
    "ipca = IncrementalPCA(n_components = size, batch_size = 500)\n",
    "for batch in np.array_split(scaled_features, len(features) // 500):\n",
    "    ipca.partial_fit(batch)\n",
    "\n",
    "print(f'information retained: {sum(ipca.explained_variance_ratio_):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTOr8TbKqZWw"
   },
   "outputs": [],
   "source": [
    "transformed_features = ipca.transform(scaled_features)\n",
    "new_data = pd.DataFrame(transformed_features, columns = [f'PC{i+1}' for i in range(size)])\n",
    "new_data['Attack Type'] = attacks.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "qe-2NiIhSvdM",
    "outputId": "22f2b674-27f6-4295-dd8f-01603c145152"
   },
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_y73RFP274P"
   },
   "source": [
    "## 4.\tMachine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tgkPDHR59TJ"
   },
   "source": [
    "### Each of the model descriptions is written in their designated sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxBJKYIYbspe"
   },
   "outputs": [],
   "source": [
    "# For cross validation\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Db-kcdZVVrYX"
   },
   "source": [
    "### Creating a Balanced Dataset for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NedYVzvrAkys"
   },
   "source": [
    "We know that a balanced dataset is crucial in machine learning because it\n",
    "ensures that each class or category of data is represented equally. This means that the number of observations in each class is roughly the same which prevents the model from being biased toward the majority class. A biased dataset can lead to poor model performance, as the model may have difficulty predicting the minority classes. As we already know that the following dataset is highly imbalanced, we took the help of **SMOTE (Synthetic Minority Over-sampling Technique)** to upsample the minority classes while creating a balanced dataset for multi-class classification. This helped us in creating an overall balanced dataset to feed the classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b3IY9PKxVyK6",
    "outputId": "ce9230ff-4dab-4ac4-b604-2b357a80cc7d"
   },
   "outputs": [],
   "source": [
    "# Creating a balanced dataset for Binary Classification\n",
    "normal_traffic = new_data.loc[new_data['Attack Type'] == 'BENIGN']\n",
    "intrusions = new_data.loc[new_data['Attack Type'] != 'BENIGN']\n",
    "\n",
    "normal_traffic = normal_traffic.sample(n = len(intrusions), replace = False)\n",
    "\n",
    "ids_data = pd.concat([intrusions, normal_traffic])\n",
    "ids_data['Attack Type'] = np.where((ids_data['Attack Type'] == 'BENIGN'), 0, 1)\n",
    "bc_data = ids_data.sample(n = 15000)\n",
    "\n",
    "print(bc_data['Attack Type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50MzC2G5V0PP"
   },
   "outputs": [],
   "source": [
    "# Splitting the data into features (X) and target (y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_bc = bc_data.drop('Attack Type', axis = 1)\n",
    "y_bc = bc_data['Attack Type']\n",
    "\n",
    "X_train_bc, X_test_bc, y_train_bc, y_test_bc = train_test_split(X_bc, y_bc, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHpB4fGaTzPK"
   },
   "source": [
    "### Logistic Regression (Binary Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_Dq3tp7HmhA"
   },
   "source": [
    "Logistic regression is a type of statistical model used to predict the probability of a binary outcome based on one or more independent variables. It models the relationship between the independent and dependent variable using a sigmoid function to output a probability score between 0 and 1. It's often used in classification tasks where the goal is to determine which of two classes an observation belongs to, such as whether an email is spam or not.\n",
    "\\\n",
    "\\\n",
    "**Parameters:**\\\n",
    "*max_iter:* this parameter sets the maximum number of iterations for the solver to converge. The default value is set to 100. However, our model could not converge with only 100 iterations so we increased it to our desire.\\\n",
    "\\\n",
    "*C:* This parameter is the regularization strength and controls the trade-off between fitting the training data well and avoiding overfitting. A smaller value of C specifies stronger regularization. We used a lower value for one model and higher value on other to see how the models perform in avoiding overfitting after placing high and low importance respectively.\\\n",
    "\\\n",
    "*solver:* This parameter specifies the algorithm to use in the optimization problem when fitting the logistic regression model. There are several different solver algorithms available such as lbfgs, saga, liblinear and a few more. We went with 'saga' and 'sag' to train our models.\\\n",
    "\\\n",
    "*random_state:* This is to ensure that the output is deterministic and can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ofQqIitbqrr",
    "outputId": "49264002-1482-4f0e-bddd-4f2e47601d19"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr1 = LogisticRegression(max_iter = 10000, C = 0.1, random_state = 0, solver = 'saga')\n",
    "lr1.fit(X_train_bc, y_train_bc)\n",
    "\n",
    "cv_lr1 = cross_val_score(lr1, X_train_bc, y_train_bc, cv = 5)\n",
    "print('Logistic regression Model 1')\n",
    "print(f'\\nCross-validation scores:', ', '.join(map(str, cv_lr1)))\n",
    "print(f'\\nMean cross-validation score: {cv_lr1.mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1txdSFbC12rz",
    "outputId": "a0d1042f-b720-4c92-f729-c5610b65671b"
   },
   "outputs": [],
   "source": [
    "print('Logistic Regression Model 1 coefficients:')\n",
    "print(*lr1.coef_, sep = ', ')\n",
    "print('\\nLogistic Regression Model 1 intercept:', *lr1.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C81rVfrK57wB",
    "outputId": "4564cc9a-d208-4196-c106-9308118d3262"
   },
   "outputs": [],
   "source": [
    "lr2 = LogisticRegression(max_iter = 15000, solver = 'sag', C = 100, random_state = 0)\n",
    "lr2.fit(X_train_bc, y_train_bc)\n",
    "\n",
    "cv_lr2 = cross_val_score(lr2, X_train_bc, y_train_bc, cv = 5)\n",
    "print('Logistic regression Model 2')\n",
    "print(f'\\nCross-validation scores:', ', '.join(map(str, cv_lr2)))\n",
    "print(f'\\nMean cross-validation score: {cv_lr2.mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zc2aGXzq6eRo",
    "outputId": "f24ddbce-3868-4d44-d0b5-6119e64a3d02"
   },
   "outputs": [],
   "source": [
    "print('Logistic Regression Model 2 coefficients:')\n",
    "print(*lr2.coef_, sep = ', ')\n",
    "print('\\nLogistic Regression Model 2 intercept:', *lr2.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bYBt91SVl2Q"
   },
   "source": [
    "### Support Vector Machine (Binary Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjREE3A2TCWw"
   },
   "source": [
    "Support Vector Machine (SVM) is a type of supervised machine learning algorithm used for classification and regression analysis. It works by finding a hyperplane in a high-dimensional space that best separates the data points into different classes.\n",
    "\\\n",
    "\\\n",
    "**Parameters:**\\\n",
    "*kernel:* The kernel parameter specifies the type of kernel function to use. In this case, we have used rbf and poly kernel.\\\n",
    "\\\n",
    "*C:* The C parameter controls the trade-off between maximizing the margin and minimizing the classification error.\\\n",
    "\\\n",
    "*gamma:* The gamma parameter is a hyperparameter that determines the influence of a single training example on the decision boundary.\\\n",
    "\\\n",
    "*random_state:* This is to ensure that the output is deterministic and can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DDS7K7l7Vqll",
    "outputId": "066f6feb-973d-44de-e27a-26830e8ea7ab"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm1 = SVC(kernel = 'poly', C = 1, random_state = 0, probability = True)\n",
    "svm1.fit(X_train_bc, y_train_bc)\n",
    "\n",
    "cv_svm1 = cross_val_score(svm1, X_train_bc, y_train_bc, cv = 5)\n",
    "print('Support Vector Machine Model 1')\n",
    "print(f'\\nCross-validation scores:', ', '.join(map(str, cv_svm1)))\n",
    "print(f'\\nMean cross-validation score: {cv_svm1.mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bVsX6rNVrLY",
    "outputId": "0feb59e1-b42f-4545-9d16-a8561d18968b"
   },
   "outputs": [],
   "source": [
    "svm2 = SVC(kernel = 'rbf', C = 1, gamma = 0.1, random_state = 0, probability = True)\n",
    "svm2.fit(X_train_bc, y_train_bc)\n",
    "\n",
    "cv_svm2 = cross_val_score(svm2, X_train_bc, y_train_bc, cv = 5)\n",
    "print('Support Vector Machine Model 2')\n",
    "print(f'\\nCross-validation scores:', ', '.join(map(str, cv_svm2)))\n",
    "print(f'\\nMean cross-validation score: {cv_svm2.mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oLX5osnVuCxk",
    "outputId": "9549e267-6f68-4522-e29a-27cecc2ac055"
   },
   "outputs": [],
   "source": [
    "print('SVM Model 1 intercept:', *svm1.intercept_)\n",
    "print('SVM Model 2 intercept:', *svm2.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOZmLQQ4t730"
   },
   "source": [
    "**We did not use the linear kernel. Hence no coefficients.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bvab0SkCfy0d"
   },
   "source": [
    "### Creating a Balanced Dataset for Multi-class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MRGWKYwPJeek",
    "outputId": "e18bcb30-040f-44bf-f9fb-f1440e075da3"
   },
   "outputs": [],
   "source": [
    "new_data['Attack Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnCaEsAjA1p3",
    "outputId": "dadaac73-caf1-48ad-a65e-9f6390828962"
   },
   "outputs": [],
   "source": [
    "class_counts = new_data['Attack Type'].value_counts()\n",
    "selected_classes = class_counts[class_counts > 1950]\n",
    "class_names = selected_classes.index\n",
    "selected = new_data[new_data['Attack Type'].isin(class_names)]\n",
    "\n",
    "dfs = []\n",
    "for name in class_names:\n",
    "  df = selected[selected['Attack Type'] == name]\n",
    "  if len(df) > 2500:\n",
    "    df = df.sample(n = 5000, random_state = 0)\n",
    "\n",
    "  dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index = True)\n",
    "df['Attack Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gXDuMQ3euiM_",
    "outputId": "d1c0e5f4-0008-4e33-8fd3-433b88df337c"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = df.drop('Attack Type', axis=1)\n",
    "y = df['Attack Type']\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=0)\n",
    "X_upsampled, y_upsampled = smote.fit_resample(X, y)\n",
    "\n",
    "blnc_data = pd.DataFrame(X_upsampled)\n",
    "blnc_data['Attack Type'] = y_upsampled\n",
    "blnc_data = blnc_data.sample(frac=1)\n",
    "\n",
    "blnc_data['Attack Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W958TQwtSxgz"
   },
   "outputs": [],
   "source": [
    "features = blnc_data.drop('Attack Type', axis = 1)\n",
    "labels = blnc_data['Attack Type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isyblWcu-wmg"
   },
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "susNMd2vOO4M"
   },
   "source": [
    "Random Forest is an ensemble learning method that combines multiple decision trees to improve the accuracy and generalization performance of the model. The basic idea behind random forests is to fit multiple decision trees on random subsets of the training data and average their predictions to reduce overfitting and improve generalization performance.\n",
    "\\\n",
    "\\\n",
    "**Parameters:**\\\n",
    "*n_estimators:* This parameter specifies the number of decision trees to fit in the random forest.\\\n",
    "\\\n",
    "*max_depth:* This parameter specifies the maximum depth of each decision tree in the random forest. A deeper tree can capture more complex interactions in the data. In our case, this parameter played a major role getting better results.\\\n",
    "\\\n",
    "*max_features:* This parameter specifies the number of features to consider when looking for the best split in each tree. We trained the first model taking all the features into account an dfor the second one, we used only 20 features.\\\n",
    "\\\n",
    "*random_state:* As mentioned earliar, this is to ensure that the output is deterministic and can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eK2A_QdRTaV0",
    "outputId": "3c949e70-85b1-460b-aa5a-410b405eb639"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf1 = RandomForestClassifier(n_estimators = 10, max_depth = 6, max_features = None, random_state = 0)\n",
    "rf1.fit(X_train, y_train)\n",
    "\n",
    "cv_rf1 = cross_val_score(rf1, X_train, y_train, cv = 5)\n",
    "print('Random Forest Model 1')\n",
    "print(f'\\nCross-validation scores:', ', '.join(map(str, cv_rf1)))\n",
    "print(f'\\nMean cross-validation score: {cv_rf1.mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_H0qB7aUFKj",
    "outputId": "a8475fc1-4963-4a5b-8271-4a5d584d4b3c"
   },
   "outputs": [],
   "source": [
    "rf2 = RandomForestClassifier(n_estimators = 15, max_depth = 8, max_features = 20, random_state = 0)\n",
    "rf2.fit(X_train, y_train)\n",
    "\n",
    "cv_rf2 = cross_val_score(rf2, X_train, y_train, cv = 5)\n",
    "print('Random Forest Model 2')\n",
    "print(f'\\nCross-validation scores:', ', '.join(map(str, cv_rf2)))\n",
    "print(f'\\nMean cross-validation score: {cv_rf2.mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D1ip-J88_fE"
   },
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTODuqEDQh3p"
   },
   "source": [
    "A decision tree is a type of algorithm used in machine learning for both classification and regression tasks. The algorithm works by recursively splitting the data into smaller subsets based on the values of the input features until a stopping criterion is met. In our case, it's the maximum depth of the tree.\n",
    "\\\n",
    "\\\n",
    "**Parameters:**\\\n",
    "*max_depth:* This parameter specifies the maximum depth of the tree. A deeper tree can capture more complex interactions in the data but can be computationally expensive. We started with a small depth and later increased it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gGhd5swq9J5G",
    "outputId": "70a51cac-b6f4-4e75-d20e-cf2bbce7ba42"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt1 = DecisionTreeClassifier(max_depth = 6)\n",
    "dt1.fit(X_train, y_train)\n",
    "\n",
    "cv_dt1 = cross_val_score(dt1, X_train, y_train, cv = 5)\n",
    "print('Decision Tree Model 1')\n",
    "print(f'\\nCross-validation scores:', ', '.join(map(str, cv_dt1)))\n",
    "print(f'\\nMean cross-validation score: {cv_dt1.mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_VG63XESTmgK",
    "outputId": "cfb988e4-4cf3-4fbb-d644-1b07fe2c2db0"
   },
   "outputs": [],
   "source": [
    "dt2 = DecisionTreeClassifier(max_depth = 8)\n",
    "dt2.fit(X_train, y_train)\n",
    "\n",
    "cv_dt2 = cross_val_score(dt2, X_train, y_train, cv = 5)\n",
    "print('Decision Tree Model 2')\n",
    "print(f'\\nCross-validation scores:', ', '.join(map(str, cv_dt2)))\n",
    "print(f'\\nMean cross-validation score: {cv_dt2.mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qr2iEpQYjmr"
   },
   "source": [
    "### K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8IS9CyqsBa8"
   },
   "source": [
    "K Nearest Neighbors (KNN) is a simple algorithm that searches for the k closest data points (neighbors) in the training set to the new input data point, based on some distance metric, usually Euclidean distance. Then, the algorithm takes a majority vote for classification of the labels or target values of those k neighbors to predict the label or target value of the new data point.\n",
    "\\\n",
    "\\\n",
    "**Parameters:**\\\n",
    "*n_neighbors:* This is a hyperparameter of the KNN algorithm that specifies the number of neighbors to consider when making predictions for a new input data point. In our case we initailly started with 16 to make predictions. So, the model will consider the 16 closest data points (neighbors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bZqyPHBAYuIF",
    "outputId": "703e8e00-bf0e-4bb0-97a7-f8165cb33689"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn1 = KNeighborsClassifier(n_neighbors = 16)\n",
    "knn1.fit(X_train, y_train)\n",
    "\n",
    "cv_knn1 = cross_val_score(knn1, X_train, y_train, cv = 5)\n",
    "print('K Nearest Neighbors Model 1')\n",
    "print(f'\\nCross-validation scores:', ', '.join(map(str, cv_knn1)))\n",
    "print(f'\\nMean cross-validation score: {cv_knn1.mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrasZwf5Yua3",
    "outputId": "bbf2ab04-a6bf-432e-81ad-0d31beaa8505"
   },
   "outputs": [],
   "source": [
    "knn2 = KNeighborsClassifier(n_neighbors = 8)\n",
    "knn2.fit(X_train, y_train)\n",
    "\n",
    "cv_knn2 = cross_val_score(knn2, X_train, y_train, cv = 5)\n",
    "print('K Nearest Neighbors Model 1')\n",
    "print(f'\\nCross-validation scores:', ', '.join(map(str, cv_knn2)))\n",
    "print(f'\\nMean cross-validation score: {cv_knn2.mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEDT9-gO6UfJ"
   },
   "source": [
    "## 5.\tPerformance Evaluation and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_hyY4e9B1hn"
   },
   "outputs": [],
   "source": [
    "# Importing necessary functions\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report, \\\n",
    " roc_auc_score, roc_curve, auc, precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUzFJzuL7K35"
   },
   "source": [
    "### Logistic Regression Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "aZDJpxcj7JjW",
    "outputId": "d67263b9-54ac-4b15-cd0e-55f78d5a8716"
   },
   "outputs": [],
   "source": [
    "y_pred_lr1 = lr1.predict(X_test_bc)\n",
    "y_pred_lr2 = lr2.predict(X_test_bc)\n",
    "\n",
    "conf_matrix_model1 = confusion_matrix(y_test_bc, y_pred_lr1)\n",
    "conf_matrix_model2 = confusion_matrix(y_test_bc, y_pred_lr2)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize = (12, 4))\n",
    "\n",
    "sns.heatmap(conf_matrix_model1, annot = True, cmap = 'Blues', ax = axs[0])\n",
    "axs[0].set_title('Model 1')\n",
    "\n",
    "sns.heatmap(conf_matrix_model2, annot = True, cmap = 'Blues', ax = axs[1])\n",
    "axs[1].set_title('Model 2')\n",
    "\n",
    "axs[0].set_xlabel('Predicted label')\n",
    "axs[0].set_ylabel('True label')\n",
    "axs[1].set_xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "WP6blBkphNMM",
    "outputId": "3e22a31e-7d4e-415a-cf28-82758125fbb6"
   },
   "outputs": [],
   "source": [
    "y_prob_lr1 = lr1.predict_proba(X_test_bc)[:,1]\n",
    "y_prob_lr2 = lr2.predict_proba(X_test_bc)[:,1]\n",
    "\n",
    "fpr1, tpr1, _ = roc_curve(y_test_bc, y_prob_lr1)\n",
    "roc_auc1 = auc(fpr1, tpr1)\n",
    "\n",
    "fpr2, tpr2, _ = roc_curve(y_test_bc, y_prob_lr2)\n",
    "roc_auc2 = auc(fpr2, tpr2)\n",
    "\n",
    "colors = sns.color_palette('Set2', n_colors = 3)\n",
    "fig, axes = plt.subplots(1, 3, figsize = (15, 5))\n",
    "\n",
    "axes[0].plot(fpr1, tpr1, label = f'ROC curve (area = {roc_auc1:.2%})', color = colors[1])\n",
    "axes[0].plot([0, 1], [0, 1], color = colors[0], linestyle = '--')\n",
    "axes[0].set_xlim([-0.05, 1.0])\n",
    "axes[0].set_ylim([0.0, 1.05])\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve (Model 1)')\n",
    "axes[0].legend(loc = 'lower right')\n",
    "\n",
    "axes[1].plot(fpr2, tpr2, label = f'ROC curve (area = {roc_auc2:.2%})', color = colors[2])\n",
    "axes[1].plot([0, 1], [0, 1], color = colors[0], linestyle = '--')\n",
    "axes[1].set_xlim([-0.05, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve (Model 2)')\n",
    "axes[1].legend(loc = 'lower right')\n",
    "\n",
    "axes[2].plot(fpr1, tpr1, label = f'ROC curve (area = {roc_auc1:.2%})', color = colors[1])\n",
    "axes[2].plot(fpr2, tpr2, label = f'ROC curve (area = {roc_auc2:.2%})', color = colors[2])\n",
    "axes[2].plot([0, 1], [0, 1], color = colors[0], linestyle = '--')\n",
    "axes[2].set_xlim([-0.05, 1.0])\n",
    "axes[2].set_ylim([0.0, 1.05])\n",
    "axes[2].set_xlabel('False Positive Rate')\n",
    "axes[2].set_ylabel('True Positive Rate')\n",
    "axes[2].set_title('Model 1 vs Model 2')\n",
    "axes[2].legend(loc = 'lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "QBt7iRvb-QKU",
    "outputId": "38da8831-ffed-4b77-d35f-8e0fb7d0bcce"
   },
   "outputs": [],
   "source": [
    "precision1, recall1, threshold1 = precision_recall_curve(y_test_bc, y_prob_lr1)\n",
    "precision2, recall2, threshold2 = precision_recall_curve(y_test_bc, y_prob_lr2)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize = (15, 5))\n",
    "\n",
    "axs[0].plot(recall1, precision1, color = colors[1], label = 'Model 1')\n",
    "axs[0].set_xlabel('Recall')\n",
    "axs[0].set_ylabel('Precision')\n",
    "axs[0].set_title('Precision-Recall Curve (Model 1)')\n",
    "\n",
    "axs[1].plot(recall2, precision2, color = colors[2], label = 'Model 2')\n",
    "axs[1].set_xlabel('Recall')\n",
    "axs[1].set_ylabel('Precision')\n",
    "axs[1].set_title('Precision-Recall Curve (Model 2)')\n",
    "\n",
    "axs[2].plot(recall1, precision1, color = colors[1], label = 'Model 1')\n",
    "axs[2].plot(recall2, precision2, color = colors[2], label = 'Model 2')\n",
    "axs[2].set_xlabel('Recall')\n",
    "axs[2].set_ylabel('Precision')\n",
    "axs[2].set_title('Model 1 vs Model 2')\n",
    "axs[2].legend(loc = 'lower left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "r4cFymF0HMhs",
    "outputId": "de014981-2265-42b4-c82f-2550511fb88d"
   },
   "outputs": [],
   "source": [
    "target_names = lr1.classes_\n",
    "metrics1 = classification_report(y_true = y_test_bc, y_pred = y_pred_lr1, target_names = target_names, output_dict = True)\n",
    "precision1 = [metrics1[target_name]['precision'] for target_name in target_names]\n",
    "recall1 = [metrics1[target_name]['recall'] for target_name in target_names]\n",
    "f1_score1 = [metrics1[target_name]['f1-score'] for target_name in target_names]\n",
    "\n",
    "metrics2 = classification_report(y_true = y_test_bc, y_pred = y_pred_lr2, target_names = target_names, output_dict = True)\n",
    "precision2 = [metrics2[target_name]['precision'] for target_name in target_names]\n",
    "recall2 = [metrics2[target_name]['recall'] for target_name in target_names]\n",
    "f1_score2 = [metrics2[target_name]['f1-score'] for target_name in target_names]\n",
    "\n",
    "data1 = np.array([precision1, recall1, f1_score1])\n",
    "data2 = np.array([precision2, recall2, f1_score2])\n",
    "rows = ['Precision', 'Recall', 'F1-score']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize = (14, 6))\n",
    "sns.heatmap(data1, cmap='Pastel1', annot = True, fmt='.2f', xticklabels = target_names, yticklabels = rows, ax = axs[0])\n",
    "sns.heatmap(data2, cmap = 'Pastel1', annot = True, fmt = '.2f', xticklabels = target_names, yticklabels = rows, ax = axs[1])\n",
    "axs[0].set_title('Classification Report (Model 1)')\n",
    "axs[1].set_title('Classification Report (Model 2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "oExZbeARAnq_",
    "outputId": "3107fa23-088b-4ccf-fad1-25c47dfb09fa"
   },
   "outputs": [],
   "source": [
    "palette = sns.color_palette('Blues', n_colors = 3)\n",
    "\n",
    "acc1 = accuracy_score(y_pred_lr1, y_test_bc)\n",
    "acc2 = accuracy_score(y_pred_lr2, y_test_bc)\n",
    "\n",
    "labels = ['Model 1', 'Model 2']\n",
    "scores = [acc1, acc2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (9, 3))\n",
    "ax.barh(labels, scores, color = palette)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_xlabel('Accuracy Score')\n",
    "ax.set_title('Logistic Regression Model Comparison')\n",
    "\n",
    "for i, v in enumerate(scores):\n",
    "    ax.text(v + 0.01, i, str(round(v, 3)), ha = 'left', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "hocfVoCxj9Bz",
    "outputId": "4296e2d8-1bc3-43f3-dd19-3f76564770b9"
   },
   "outputs": [],
   "source": [
    "palette = sns.color_palette('Greens', n_colors = 3)\n",
    "\n",
    "labels = ['Model 1', 'Model 2']\n",
    "scores = [cv_lr1.mean(), cv_lr2.mean()]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (9, 3))\n",
    "ax.barh(labels, scores, color = palette)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_xlabel('Cross Validation Score')\n",
    "ax.set_title('Logistic Regression Model Comparison (Cross Validation)')\n",
    "\n",
    "for i, v in enumerate(scores):\n",
    "    ax.text(v + 0.01, i, str(round(v, 3)), ha = 'left', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_5dqWNjXuRL"
   },
   "source": [
    "### Support Vector Machine Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "mD8EQxRsXuRL",
    "outputId": "36d95d80-dc02-4807-b835-ca9ff322dbaa"
   },
   "outputs": [],
   "source": [
    "y_pred_svm1 = svm1.predict(X_test_bc)\n",
    "y_pred_svm2 = svm2.predict(X_test_bc)\n",
    "\n",
    "conf_matrix_model1 = confusion_matrix(y_test_bc, y_pred_svm1)\n",
    "conf_matrix_model2 = confusion_matrix(y_test_bc, y_pred_svm2)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize = (12, 4))\n",
    "\n",
    "sns.heatmap(conf_matrix_model1, annot = True, cmap = 'Blues', ax = axs[0])\n",
    "axs[0].set_title('Model 1')\n",
    "\n",
    "sns.heatmap(conf_matrix_model2, annot = True, cmap = 'Blues', ax = axs[1])\n",
    "axs[1].set_title('Model 2')\n",
    "\n",
    "axs[0].set_xlabel('Predicted label')\n",
    "axs[0].set_ylabel('True label')\n",
    "axs[1].set_xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "5Xof-5OuXuRM",
    "outputId": "2e7ad99f-d0b4-4428-8caa-202329e67c6b"
   },
   "outputs": [],
   "source": [
    "y_prob_svm1 = svm1.predict_proba(X_test_bc)[:,1]\n",
    "y_prob_svm2 = svm2.predict_proba(X_test_bc)[:,1]\n",
    "\n",
    "fpr1, tpr1, _ = roc_curve(y_test_bc, y_prob_svm1)\n",
    "roc_auc1 = auc(fpr1, tpr1)\n",
    "\n",
    "fpr2, tpr2, _ = roc_curve(y_test_bc, y_prob_svm2)\n",
    "roc_auc2 = auc(fpr2, tpr2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize = (15, 5))\n",
    "\n",
    "axes[0].plot(fpr1, tpr1, label = f'ROC curve (area = {roc_auc1:.2%})', color = colors[1])\n",
    "axes[0].plot([0, 1], [0, 1], color = colors[0], linestyle = '--')\n",
    "axes[0].set_xlim([-0.05, 1.0])\n",
    "axes[0].set_ylim([0.0, 1.05])\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve (Model 1)')\n",
    "axes[0].legend(loc = 'lower right')\n",
    "\n",
    "axes[1].plot(fpr2, tpr2, label = f'ROC curve (area = {roc_auc2:.2%})', color = colors[2])\n",
    "axes[1].plot([0, 1], [0, 1], color = colors[0], linestyle = '--')\n",
    "axes[1].set_xlim([-0.05, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve (Model 2)')\n",
    "axes[1].legend(loc = 'lower right')\n",
    "\n",
    "axes[2].plot(fpr1, tpr1, label = f'ROC curve (area = {roc_auc1:.2%})', color = colors[1])\n",
    "axes[2].plot(fpr2, tpr2, label = f'ROC curve (area = {roc_auc2:.2%})', color = colors[2])\n",
    "axes[2].plot([0, 1], [0, 1], color = colors[0], linestyle = '--')\n",
    "axes[2].set_xlim([-0.05, 1.0])\n",
    "axes[2].set_ylim([0.0, 1.05])\n",
    "axes[2].set_xlabel('False Positive Rate')\n",
    "axes[2].set_ylabel('True Positive Rate')\n",
    "axes[2].set_title('Model 1 vs Model 2')\n",
    "axes[2].legend(loc = 'lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "2ha0tlm6XuRM",
    "outputId": "fb42abe4-408a-43c1-eec9-16da48ba38db"
   },
   "outputs": [],
   "source": [
    "precision1, recall1, threshold1 = precision_recall_curve(y_test_bc, y_prob_svm1)\n",
    "precision2, recall2, threshold2 = precision_recall_curve(y_test_bc, y_prob_svm2)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize = (15, 5))\n",
    "\n",
    "axs[0].plot(recall1, precision1, color = colors[1])\n",
    "axs[0].set_xlabel('Recall')\n",
    "axs[0].set_ylabel('Precision')\n",
    "axs[0].set_title('Precision-Recall Curve (Model 1)')\n",
    "\n",
    "axs[1].plot(recall2, precision2, color = colors[2])\n",
    "axs[1].set_xlabel('Recall')\n",
    "axs[1].set_ylabel('Precision')\n",
    "axs[1].set_title('Precision-Recall Curve (Model 2)')\n",
    "\n",
    "axs[2].plot(recall1, precision1, color = colors[1], label = 'Model 1')\n",
    "axs[2].plot(recall2, precision2, color = colors[2], label = 'Model 2')\n",
    "axs[2].set_xlabel('Recall')\n",
    "axs[2].set_ylabel('Precision')\n",
    "axs[2].set_title('Model 1 vs Model 2')\n",
    "axs[2].legend(loc = 'lower left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "8coNMkjWXuRM",
    "outputId": "4cd255f3-6974-4cf0-da37-6416893c047e"
   },
   "outputs": [],
   "source": [
    "target_names = svm1.classes_\n",
    "metrics1 = classification_report(y_true = y_test_bc, y_pred = y_pred_svm1, target_names = target_names, output_dict = True)\n",
    "precision1 = [metrics1[target_name]['precision'] for target_name in target_names]\n",
    "recall1 = [metrics1[target_name]['recall'] for target_name in target_names]\n",
    "f1_score1 = [metrics1[target_name]['f1-score'] for target_name in target_names]\n",
    "\n",
    "metrics2 = classification_report(y_true = y_test_bc, y_pred = y_pred_svm2, target_names = target_names, output_dict = True)\n",
    "precision2 = [metrics2[target_name]['precision'] for target_name in target_names]\n",
    "recall2 = [metrics2[target_name]['recall'] for target_name in target_names]\n",
    "f1_score2 = [metrics2[target_name]['f1-score'] for target_name in target_names]\n",
    "\n",
    "data1 = np.array([precision1, recall1, f1_score1])\n",
    "data2 = np.array([precision2, recall2, f1_score2])\n",
    "rows = ['Precision', 'Recall', 'F1-score']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "sns.heatmap(data1, cmap = 'Pastel1', annot = True, fmt = '.2f', xticklabels = target_names, yticklabels = rows, ax = axs[0])\n",
    "sns.heatmap(data2, cmap = 'Pastel1', annot = True, fmt = '.2f', xticklabels = target_names, yticklabels = rows, ax = axs[1])\n",
    "axs[0].set_title('Classification Report (Model 1)')\n",
    "axs[1].set_title('Classification Report (Model 2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "EzNlyxISXuRN",
    "outputId": "1d8baca4-6a34-41e9-b651-e0876dad23b7"
   },
   "outputs": [],
   "source": [
    "palette = sns.color_palette('Blues', n_colors = 2)\n",
    "\n",
    "acc1 = accuracy_score(y_pred_svm1, y_test_bc)\n",
    "acc2 = accuracy_score(y_pred_svm2, y_test_bc)\n",
    "\n",
    "labels = ['Model 1', 'Model 2']\n",
    "scores = [acc1, acc2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (9, 3))\n",
    "ax.barh(labels, scores, color = palette)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_xlabel('Accuracy Score')\n",
    "ax.set_title('Support Vector Machine Model Comparison')\n",
    "\n",
    "for i, v in enumerate(scores):\n",
    "    ax.text(v + 0.01, i, str(round(v, 3)), ha = 'left', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "AUjTas0zkX_g",
    "outputId": "c3df7c5e-b103-4880-e704-b4c86d5da2ea"
   },
   "outputs": [],
   "source": [
    "palette = sns.color_palette('Greens', n_colors = 2)\n",
    "\n",
    "labels = ['Model 1', 'Model 2']\n",
    "scores = [cv_svm1.mean(), cv_svm2.mean()]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (9, 3))\n",
    "ax.barh(labels, scores, color = palette)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_xlabel('Cross Validation Score')\n",
    "ax.set_title('Support Vector Machine Model Comparison (Cross Validation)')\n",
    "\n",
    "for i, v in enumerate(scores):\n",
    "    ax.text(v + 0.01, i, str(round(v, 3)), ha = 'left', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21Q9ths2VJC-"
   },
   "source": [
    "### Comparison of the Binary Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leiWe60Qnp8Y"
   },
   "source": [
    "We trained two models for each different classification algorithm. For comparing different algorithms, we will take the best performing model from each class based on the model's precision, recall, accuracy, etc.\n",
    "\n",
    "1. Logistic Regression: Model 2\n",
    "1. Support Vector Machine: Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "E4EX44I7oVo3",
    "outputId": "84dee316-1f1b-48a8-e410-7ba71c9de5e0"
   },
   "outputs": [],
   "source": [
    "conf_matrix_model1 = confusion_matrix(y_test_bc, y_pred_lr2)\n",
    "conf_matrix_model2 = confusion_matrix(y_test_bc, y_pred_svm2)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize = (12, 4))\n",
    "\n",
    "sns.heatmap(conf_matrix_model1, annot = True, cmap = 'Blues', ax = axs[0])\n",
    "axs[0].set_title('Logistic Regression')\n",
    "\n",
    "sns.heatmap(conf_matrix_model2, annot = True, cmap = 'Blues', ax = axs[1])\n",
    "axs[1].set_title('Support Vector Machine')\n",
    "\n",
    "axs[0].set_xlabel('Predicted label')\n",
    "axs[0].set_ylabel('True label')\n",
    "axs[1].set_xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "_c-GDUcQoVo4",
    "outputId": "8aef6a67-d53e-4672-9452-04d92ef42955"
   },
   "outputs": [],
   "source": [
    "fpr1, tpr1, _ = roc_curve(y_test_bc, y_prob_lr2)\n",
    "roc_auc1 = auc(fpr1, tpr1)\n",
    "\n",
    "fpr2, tpr2, _ = roc_curve(y_test_bc, y_prob_svm2)\n",
    "roc_auc2 = auc(fpr2, tpr2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize = (15, 5))\n",
    "\n",
    "axes[0].plot(fpr1, tpr1, label = f'ROC curve (area = {roc_auc1:.2%})', color = colors[1])\n",
    "axes[0].plot([0, 1], [0, 1], color = colors[0], linestyle = '--')\n",
    "axes[0].set_xlim([-0.05, 1.0])\n",
    "axes[0].set_ylim([0.0, 1.05])\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve (Logistic Regression)')\n",
    "axes[0].legend(loc = 'lower right')\n",
    "\n",
    "axes[1].plot(fpr2, tpr2, label = f'ROC curve (area = {roc_auc2:.2%})', color = colors[2])\n",
    "axes[1].plot([0, 1], [0, 1], color = colors[0], linestyle = '--')\n",
    "axes[1].set_xlim([-0.05, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve (SVM)')\n",
    "axes[1].legend(loc = 'lower right')\n",
    "\n",
    "axes[2].plot(fpr1, tpr1, label = f'LR ROC curve (area = {roc_auc1:.2%})', color = colors[1])\n",
    "axes[2].plot(fpr2, tpr2, label = f'SVM ROC curve (area = {roc_auc2:.2%})', color = colors[2])\n",
    "axes[2].plot([0, 1], [0, 1], color = colors[0], linestyle = '--')\n",
    "axes[2].set_xlim([-0.05, 1.0])\n",
    "axes[2].set_ylim([0.0, 1.05])\n",
    "axes[2].set_xlabel('False Positive Rate')\n",
    "axes[2].set_ylabel('True Positive Rate')\n",
    "axes[2].set_title('LR vs SVM')\n",
    "axes[2].legend(loc = 'lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "yUp1C2jKoVo5",
    "outputId": "f9d064d4-c20b-493a-f5e5-4c37ff5ad0e0"
   },
   "outputs": [],
   "source": [
    "precision1, recall1, threshold1 = precision_recall_curve(y_test_bc, y_prob_lr2)\n",
    "precision2, recall2, threshold2 = precision_recall_curve(y_test_bc, y_prob_svm2)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize = (15, 5))\n",
    "\n",
    "axs[0].plot(recall1, precision1, color = colors[1], label = 'Model 1')\n",
    "axs[0].set_xlabel('Recall')\n",
    "axs[0].set_ylabel('Precision')\n",
    "axs[0].set_title('Precision-Recall Curve (LR)')\n",
    "\n",
    "axs[1].plot(recall2, precision2, color = colors[2], label = 'Model 2')\n",
    "axs[1].set_xlabel('Recall')\n",
    "axs[1].set_ylabel('Precision')\n",
    "axs[1].set_title('Precision-Recall Curve (SVM)')\n",
    "\n",
    "axs[2].plot(recall1, precision1, color = colors[1], label = 'Logistic Regression')\n",
    "axs[2].plot(recall2, precision2, color = colors[2], label = 'Support Vector Machine')\n",
    "axs[2].set_xlabel('Recall')\n",
    "axs[2].set_ylabel('Precision')\n",
    "axs[2].set_title('LR vs SVM')\n",
    "axs[2].legend(loc = 'lower left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 549
    },
    "id": "V2qkYUGFoVo5",
    "outputId": "0848acc0-54d7-46ca-c6b7-176be8379a2e"
   },
   "outputs": [],
   "source": [
    "target_names = svm2.classes_\n",
    "metrics1 = classification_report(y_true = y_test_bc, y_pred = y_pred_lr2, target_names = target_names, output_dict = True)\n",
    "precision1 = [metrics1[target_name]['precision'] for target_name in target_names]\n",
    "recall1 = [metrics1[target_name]['recall'] for target_name in target_names]\n",
    "f1_score1 = [metrics1[target_name]['f1-score'] for target_name in target_names]\n",
    "\n",
    "metrics2 = classification_report(y_true = y_test_bc, y_pred = y_pred_svm2, target_names = target_names, output_dict = True)\n",
    "precision2 = [metrics2[target_name]['precision'] for target_name in target_names]\n",
    "recall2 = [metrics2[target_name]['recall'] for target_name in target_names]\n",
    "f1_score2 = [metrics2[target_name]['f1-score'] for target_name in target_names]\n",
    "\n",
    "data1 = np.array([precision1, recall1, f1_score1])\n",
    "data2 = np.array([precision2, recall2, f1_score2])\n",
    "rows = ['Precision', 'Recall', 'F1-score']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize = (14, 6))\n",
    "sns.heatmap(data1, cmap = 'Pastel1', annot = True, fmt = '.2f', xticklabels = target_names, yticklabels = rows, ax=axs[0])\n",
    "sns.heatmap(data2, cmap = 'Pastel1', annot = True, fmt = '.2f', xticklabels = target_names, yticklabels = rows, ax=axs[1])\n",
    "axs[0].set_title('Classification Report (LR)')\n",
    "axs[1].set_title('Classification Report (SVM)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "FPDQh5PyoVo5",
    "outputId": "be8c6715-7f4a-4382-c624-e5b914b686b7"
   },
   "outputs": [],
   "source": [
    "palette = sns.color_palette('Blues', n_colors = 2)\n",
    "\n",
    "acc1 = accuracy_score(y_pred_lr2, y_test_bc)\n",
    "acc2 = accuracy_score(y_pred_svm2, y_test_bc)\n",
    "\n",
    "labels = ['Logistic Regression', 'Support Vector Machine']\n",
    "scores = [acc1, acc2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (9, 3))\n",
    "ax.barh(labels, scores, color = palette)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_xlabel('Accuracy Score')\n",
    "ax.set_title('Binary Classification Model Comparison')\n",
    "\n",
    "for i, v in enumerate(scores):\n",
    "    ax.text(v + 0.01, i, str(round(v, 3)), ha = 'left', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "MaXXUOcsr2xJ",
    "outputId": "41bf010b-93e0-448f-bd8a-cc8bd5c6f62d"
   },
   "outputs": [],
   "source": [
    "palette = sns.color_palette('Greens', n_colors = 2)\n",
    "\n",
    "labels = ['Logistic Regression', 'Support Vector Machine']\n",
    "scores = [cv_lr2.mean(), cv_svm2.mean()]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (9, 3))\n",
    "ax.barh(labels, scores, color = palette)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_xlabel('Cross Validation Score')\n",
    "ax.set_title('Binary Classification Model Comparison')\n",
    "\n",
    "for i, v in enumerate(scores):\n",
    "    ax.text(v + 0.01, i, str(round(v, 3)), ha = 'left', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDp-xQWSqtLV"
   },
   "source": [
    "### Random Forest Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "K3geU-g0qxqx",
    "outputId": "c06c9742-77f1-4de2-c15d-ac97df4d9a59"
   },
   "outputs": [],
   "source": [
    "y_pred_rf1 = rf1.predict(X_test)\n",
    "y_pred_rf2 = rf2.predict(X_test)\n",
    "\n",
    "conf_matrix_model1 = confusion_matrix(y_test, y_pred_rf1)\n",
    "conf_matrix_model2 = confusion_matrix(y_test, y_pred_rf2)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize = (16, 7))\n",
    "\n",
    "sns.heatmap(conf_matrix_model1, annot = True, cmap = 'Blues', ax = axs[0], xticklabels = rf1.classes_, yticklabels = rf1.classes_)\n",
    "axs[0].set_title('Model 1')\n",
    "\n",
    "sns.heatmap(conf_matrix_model2, annot = True, cmap = 'Blues', ax = axs[1], xticklabels = rf2.classes_, yticklabels = rf2.classes_)\n",
    "axs[1].set_title('Model 2')\n",
    "\n",
    "axs[0].set_xlabel('Predicted label')\n",
    "axs[0].set_ylabel('True label')\n",
    "axs[1].set_xlabel('Predicted label')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "kgjmfsmQJd7w",
    "outputId": "406ff818-b47e-4108-f5d4-0315fa1262ac"
   },
   "outputs": [],
   "source": [
    "target_names = rf1.classes_\n",
    "metrics1 = classification_report(y_true = y_test, y_pred = y_pred_rf1, target_names = target_names, output_dict = True)\n",
    "precision1 = [metrics1[target_name]['precision'] for target_name in target_names]\n",
    "recall1 = [metrics1[target_name]['recall'] for target_name in target_names]\n",
    "f1_score1 = [metrics1[target_name]['f1-score'] for target_name in target_names]\n",
    "\n",
    "metrics2 = classification_report(y_true = y_test, y_pred = y_pred_rf2, target_names = target_names, output_dict = True)\n",
    "precision2 = [metrics2[target_name]['precision'] for target_name in target_names]\n",
    "recall2 = [metrics2[target_name]['recall'] for target_name in target_names]\n",
    "f1_score2 = [metrics2[target_name]['f1-score'] for target_name in target_names]\n",
    "\n",
    "data1 = np.array([precision1, recall1, f1_score1])\n",
    "data2 = np.array([precision2, recall2, f1_score2])\n",
    "rows = ['Precision', 'Recall', 'F1-score']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize = (14, 6))\n",
    "sns.heatmap(data1, cmap = 'Pastel1', annot = True, fmt = '.2f', xticklabels = target_names, yticklabels = rows, ax = axs[0])\n",
    "sns.heatmap(data2, cmap = 'Pastel1', annot = True, fmt = '.2f', xticklabels = target_names, yticklabels = rows, ax = axs[1])\n",
    "axs[0].set_title('Classification Report (Model 1)')\n",
    "axs[1].set_title('Classification Report (Model 2)')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "-AEq7AxGwHb3",
    "outputId": "27112d37-cca7-46ef-b61d-a7a8e1c3891b"
   },
   "outputs": [],
   "source": [
    "palette = sns.color_palette('Blues', n_colors = 2)\n",
    "\n",
    "acc1 = accuracy_score(y_pred_rf1, y_test)\n",
    "acc2 = accuracy_score(y_pred_rf2, y_test)\n",
    "\n",
    "labels = ['Model 1', 'Model 2']\n",
    "scores = [acc1, acc2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (9, 3))\n",
    "ax.barh(labels, scores, color = palette)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_xlabel('Accuracy Score')\n",
    "ax.set_title('Random Forest Model Comparison')\n",
    "\n",
    "for i, v in enumerate(scores):\n",
    "    ax.text(v + 0.01, i, str(round(v, 3)), ha = 'left', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "dHewLYnurYGu",
    "outputId": "2bcb721b-e625-4568-8824-ec1889b9eca1"
   },
   "outputs": [],
   "source": [
    "palette = sns.color_palette('Greens', n_colors = 2)\n",
    "\n",
    "labels = ['Model 1', 'Model 2']\n",
    "scores = [cv_rf1.mean(), cv_rf2.mean()]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (9, 3))\n",
    "ax.barh(labels, scores, color = palette)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_xlabel('Cross Validation Score')\n",
    "ax.set_title('Support Vector Machine Model Comparison (Cross Validation)')\n",
    "\n",
    "for i, v in enumerate(scores):\n",
    "    ax.text(v + 0.01, i, str(round(v, 3)), ha = 'left', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIa0LHYKT78a"
   },
   "source": [
    "### Decision Trees Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ItDaKuqKT78b",
    "outputId": "ed6cdb8e-0493-4b13-f45f-5385d0ddf321"
   },
   "outputs": [],
   "source": [
    "y_pred_dt1 = dt1.predict(X_test)\n",
    "y_pred_dt2 = dt2.predict(X_test)\n",
    "\n",
    "conf_matrix_model1 = confusion_matrix(y_test, y_pred_dt1)\n",
    "conf_matrix_model2 = confusion_matrix(y_test, y_pred_dt2)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize = (16, 7))\n",
    "\n",
    "sns.heatmap(conf_matrix_model1, annot = True, cmap = 'Blues', ax = axs[0], xticklabels = dt1.classes_, yticklabels = dt1.classes_)\n",
    "axs[0].set_title('Model 1')\n",
    "\n",
    "sns.heatmap(conf_matrix_model2, annot = True, cmap = 'Blues', ax = axs[1], xticklabels = dt2.classes_, yticklabels = dt2.classes_)\n",
    "axs[1].set_title('Model 2')\n",
    "\n",
    "axs[0].set_xlabel('Predicted label')\n",
    "axs[0].set_ylabel('True label')\n",
    "axs[1].set_xlabel('Predicted label')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ppzpJ6NlT78b",
    "outputId": "a84b49cf-3a0c-4cc3-9ffd-caf8b1e87c37"
   },
   "outputs": [],
   "source": [
    "target_names = dt1.classes_\n",
    "metrics1 = classification_report(y_true = y_test, y_pred = y_pred_dt1, target_names = target_names, output_dict = True)\n",
    "precision1 = [metrics1[target_name]['precision'] for target_name in target_names]\n",
    "recall1 = [metrics1[target_name]['recall'] for target_name in target_names]\n",
    "f1_score1 = [metrics1[target_name]['f1-score'] for target_name in target_names]\n",
    "\n",
    "metrics2 = classification_report(y_true = y_test, y_pred = y_pred_dt2, target_names = target_names, output_dict = True)\n",
    "precision2 = [metrics2[target_name]['precision'] for target_name in target_names]\n",
    "recall2 = [metrics2[target_name]['recall'] for target_name in target_names]\n",
    "f1_score2 = [metrics2[target_name]['f1-score'] for target_name in target_names]\n",
    "\n",
    "data1 = np.array([precision1, recall1, f1_score1])\n",
    "data2 = np.array([precision2, recall2, f1_score2])\n",
    "rows = ['Precision', 'Recall', 'F1-score']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize = (14, 6))\n",
    "sns.heatmap(data1, cmap = 'Pastel1', annot = True, fmt = '.2f', xticklabels = target_names, yticklabels = rows, ax = axs[0])\n",
    "sns.heatmap(data2, cmap = 'Pastel1', annot = True, fmt = '.2f', xticklabels = target_names, yticklabels = rows, ax = axs[1])\n",
    "axs[0].set_title('Classification Report (Model 1)')\n",
    "axs[1].set_title('Classification Report (Model 2)')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Eaq6mTc6T78b",
    "outputId": "8ad21ae5-7241-484a-c854-70e8d8f56670"
   },
   "outputs": [],
   "source": [
    "palette = sns.color_palette('Blues', n_colors = 2)\n",
    "\n",
    "acc1 = accuracy_score(y_pred_dt1, y_test)\n",
    "acc2 = accuracy_score(y_pred_dt2, y_test)\n",
    "\n",
    "labels = ['Model 1', 'Model 2']\n",
    "scores = [acc1, acc2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (9, 3))\n",
    "ax.barh(labels, scores, color = palette)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_xlabel('Accuracy Score')\n",
    "ax.set_title('Decision Trees Model Comparison')\n",
    "\n",
    "for i, v in enumerate(scores):\n",
    "    ax.text(v + 0.01, i, str(round(v, 3)), ha = 'left', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "gapIkuxus3cb",
    "outputId": "74b91fe7-ffd0-4584-b61e-44e9c96433a3"
   },
   "outputs": [],
   "source": [
    "palette = sns.color_palette('Greens', n_colors = 2)\n",
    "\n",
    "labels = ['Model 1', 'Model 2']\n",
    "scores = [cv_dt1.mean(), cv_dt2.mean()]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (9, 3))\n",
    "ax.barh(labels, scores, color = palette)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_xlabel('Cross Validation Score')\n",
    "ax.set_title('Decision Trees Model Comparison (Cross Validation)')\n",
    "\n",
    "for i, v in enumerate(scores):\n",
    "    ax.text(v + 0.01, i, str(round(v, 3)), ha = 'left', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCVzGYzwXivR"
   },
   "source": [
    "### K Nearest Neighbours Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "bsItgyOIXivS",
    "outputId": "ac3b10c6-d7fc-45ab-98f4-8002b7e492e7"
   },
   "outputs": [],
   "source": [
    "y_pred_knn1 = knn1.predict(X_test)\n",
    "y_pred_knn2 = knn2.predict(X_test)\n",
    "\n",
    "conf_matrix_model1 = confusion_matrix(y_test, y_pred_knn1)\n",
    "conf_matrix_model2 = confusion_matrix(y_test, y_pred_knn2)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize = (16, 7))\n",
    "\n",
    "sns.heatmap(conf_matrix_model1, annot = True, cmap = 'Blues', ax = axs[0], xticklabels = knn1.classes_, yticklabels = knn1.classes_)\n",
    "axs[0].set_title('Model 1')\n",
    "\n",
    "sns.heatmap(conf_matrix_model2, annot = True, cmap = 'Blues', ax = axs[1], xticklabels = knn2.classes_, yticklabels = knn2.classes_)\n",
    "axs[1].set_title('Model 2')\n",
    "\n",
    "axs[0].set_xlabel('Predicted label')\n",
    "axs[0].set_ylabel('True label')\n",
    "axs[1].set_xlabel('Predicted label')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "i0W9cPGiXivT",
    "outputId": "88bcfd44-38ef-4fc9-8caf-abde0504254d"
   },
   "outputs": [],
   "source": [
    "target_names = knn1.classes_\n",
    "metrics1 = classification_report(y_true = y_test, y_pred = y_pred_knn1, target_names = target_names, output_dict = True)\n",
    "precision1 = [metrics1[target_name]['precision'] for target_name in target_names]\n",
    "recall1 = [metrics1[target_name]['recall'] for target_name in target_names]\n",
    "f1_score1 = [metrics1[target_name]['f1-score'] for target_name in target_names]\n",
    "\n",
    "metrics2 = classification_report(y_true = y_test, y_pred = y_pred_knn2, target_names = target_names, output_dict = True)\n",
    "precision2 = [metrics2[target_name]['precision'] for target_name in target_names]\n",
    "recall2 = [metrics2[target_name]['recall'] for target_name in target_names]\n",
    "f1_score2 = [metrics2[target_name]['f1-score'] for target_name in target_names]\n",
    "\n",
    "data1 = np.array([precision1, recall1, f1_score1])\n",
    "data2 = np.array([precision2, recall2, f1_score2])\n",
    "rows = ['Precision', 'Recall', 'F1-score']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize = (14, 6))\n",
    "sns.heatmap(data1, cmap = 'Pastel1', annot = True, fmt = '.2f', xticklabels = target_names, yticklabels = rows, ax = axs[0])\n",
    "sns.heatmap(data2, cmap = 'Pastel1', annot = True, fmt = '.2f', xticklabels = target_names, yticklabels = rows, ax = axs[1])\n",
    "axs[0].set_title('Classification Report (Model 1)')\n",
    "axs[1].set_title('Classification Report (Model 2)')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "-0WSwL6YXivT",
    "outputId": "359031eb-0ffb-45c3-f2f0-72ac15dfdcdc"
   },
   "outputs": [],
   "source": [
    "palette = sns.color_palette('Blues', n_colors = 2)\n",
    "\n",
    "acc1 = accuracy_score(y_pred_knn1, y_test)\n",
    "acc2 = accuracy_score(y_pred_knn2, y_test)\n",
    "\n",
    "labels = ['Model 1', 'Model 2']\n",
    "scores = [acc1, acc2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (9, 3))\n",
    "ax.barh(labels, scores, color = palette)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_xlabel('Accuracy Score')\n",
    "ax.set_title('K Nearest Neighbour Model Comparison')\n",
    "\n",
    "for i, v in enumerate(scores):\n",
    "    ax.text(v + 0.01, i, str(round(v, 3)), ha = 'left', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "j8ZSQVAPtF1S",
    "outputId": "66cf1408-3800-4bb6-d9be-3e7b3e5f445b"
   },
   "outputs": [],
   "source": [
    "palette = sns.color_palette('Greens', n_colors = 2)\n",
    "\n",
    "labels = ['Model 1', 'Model 2']\n",
    "scores = [cv_knn1.mean(), cv_knn2.mean()]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (9, 3))\n",
    "ax.barh(labels, scores, color = palette)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_xlabel('Cross Validation Score')\n",
    "ax.set_title('Decision Trees Model Comparison (Cross Validation)')\n",
    "\n",
    "for i, v in enumerate(scores):\n",
    "    ax.text(v + 0.01, i, str(round(v, 3)), ha = 'left', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxUFSY5JapE5"
   },
   "source": [
    "### Comparison of the Multi-class Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xln5XE3va7lL"
   },
   "source": [
    "We trained two models for each different classification algorithm. For comparing different algorithms, we will take the best performing model from each class based on the model's precision, recall, accuracy, etc.\n",
    "\n",
    "1. Random Forest: Model 2\n",
    "1. Decision Trees: Model 2\n",
    "1. KNN: Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "ZYl_-kxHa0Tj",
    "outputId": "fb44563e-4974-4550-e322-f23fc6e39824"
   },
   "outputs": [],
   "source": [
    "palette = sns.color_palette('Blues', n_colors = 3)\n",
    "\n",
    "rf_acc = accuracy_score(y_pred_rf2, y_test)\n",
    "dt_acc = accuracy_score(y_pred_dt2, y_test)\n",
    "knn_acc = accuracy_score(y_pred_knn2, y_test)\n",
    "\n",
    "labels = ['Random Forest', 'Decision Trees', 'K Nearest Neighbours']\n",
    "scores = [rf_acc, dt_acc, knn_acc]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (9, 3))\n",
    "ax.barh(labels, scores, color = palette)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_xlabel('Accuracy Score')\n",
    "ax.set_title('Multi-class Classification Model Comparison')\n",
    "\n",
    "for i, v in enumerate(scores):\n",
    "    ax.text(v + 0.01, i, str(round(v, 4)), ha = 'left', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "7FBhxJYjtnCP",
    "outputId": "bb55fda7-686b-4650-854d-1f78809eaca2"
   },
   "outputs": [],
   "source": [
    "palette = sns.color_palette('Greens', n_colors = 3)\n",
    "\n",
    "labels = ['Random Forest', 'Decision Trees', 'K Nearest Neighbours']\n",
    "scores = [cv_rf2.mean(), cv_dt2.mean(), cv_knn2.mean()]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (9, 3))\n",
    "ax.barh(labels, scores, color = palette)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_xlabel('Cross Validation Score')\n",
    "ax.set_title('Multi-class Classification Model Comparison')\n",
    "\n",
    "for i, v in enumerate(scores):\n",
    "    ax.text(v + 0.01, i, str(round(v, 4)), ha = 'left', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "WekRRRWU2K5M",
    "outputId": "375b526a-7eab-4342-b8e0-bc337c483109"
   },
   "outputs": [],
   "source": [
    "target_names = rf2.classes_\n",
    "preds = [y_pred_rf2, y_pred_dt2, y_pred_knn2]\n",
    "\n",
    "datas = []\n",
    "for pred in preds:\n",
    "    metrics = classification_report(y_true = y_test, y_pred = pred, target_names = target_names, output_dict = True)\n",
    "    precision = [metrics[target_name]['precision'] for target_name in target_names]\n",
    "    recall = [metrics[target_name]['recall'] for target_name in target_names]\n",
    "    f1_score = [metrics[target_name]['f1-score'] for target_name in target_names]\n",
    "\n",
    "    datas.append(np.array([precision, recall, f1_score]))\n",
    "\n",
    "rows = ['Precision', 'Recall', 'F1-score']\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize = (19, 6))\n",
    "sns.heatmap(datas[0], cmap = 'Pastel1', annot = True, fmt = '.2f', xticklabels = target_names, yticklabels = rows, ax = axs[0])\n",
    "sns.heatmap(datas[1], cmap = 'Pastel1', annot = True, fmt = '.2f', xticklabels = target_names, yticklabels = rows, ax = axs[1])\n",
    "sns.heatmap(datas[2], cmap = 'Pastel1', annot = True, fmt = '.2f', xticklabels = target_names, yticklabels = rows, ax = axs[2])\n",
    "\n",
    "axs[0].set_title('Classification Report (Random Forest)')\n",
    "axs[1].set_title('Classification Report (Decision Trees)')\n",
    "axs[2].set_title('Classification Report (K Nearest Neighbours)')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "oxS6tASTOTxf",
    "outputId": "be69a3a7-831d-4958-9648-3980b44735a2"
   },
   "outputs": [],
   "source": [
    "preds = [y_pred_rf2, y_pred_dt2, y_pred_knn2]\n",
    "\n",
    "conf_matrix = [confusion_matrix(y_test, y_pred) for y_pred in preds]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize = (22, 8))\n",
    "\n",
    "sns.heatmap(conf_matrix[0], annot = True, cmap = 'Blues', ax = axs[0], xticklabels = dt1.classes_, yticklabels = dt1.classes_)\n",
    "sns.heatmap(conf_matrix[1], annot = True, cmap = 'Blues', ax = axs[1], xticklabels = dt1.classes_, yticklabels = dt1.classes_)\n",
    "sns.heatmap(conf_matrix[2], annot = True, cmap = 'Blues', ax = axs[2], xticklabels = dt1.classes_, yticklabels = dt1.classes_)\n",
    "\n",
    "axs[0].set_title('Confusion Matrix (Random Forest)')\n",
    "axs[1].set_title('Confusion Matrix (Decision Trees)')\n",
    "axs[2].set_title('Confusion Matrix (K Nearest Neighbours)')\n",
    "\n",
    "axs[0].set_xlabel('Predicted label')\n",
    "axs[1].set_xlabel('Predicted label')\n",
    "axs[2].set_xlabel('Predicted label')\n",
    "axs[0].set_ylabel('True label')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRLhtjzD66ZG"
   },
   "source": [
    "During the training phase of various classification algorithms, we trained two models for each of the algorithm using different parameters.\n",
    "\n",
    "\n",
    "***Binary Classification Algorithms:***\n",
    "1. Logistic Regression\n",
    "2. Support Vector Machine\n",
    "\n",
    "For comapring different classification algorithms, we took the best performing models from each algorithms.\n",
    "\n",
    "Based on our various tests, we found out that logistic Regression can handle large size data and can be trained in a very short amount of time. However, the drawback is less accurate models. On the other hand, SVM is quite computationally expensive and takes a hefty amount of time to train. but the good thing is the accuracy score is much higher than the logistic regression models. We tuned a few parameters here and there to improve the models' accuracy. We also did cross-validation to make sure that our model was not overfitted and was trained just right.\n",
    "\n",
    "One other thing to mention is that the accuracy of the models is dependent on the standardization of the dataset. We trained our model with and without the use of a standard scaler. During our testing different models, we found out that the accuracy and other performance measure scores went significantly high after standardizing the dataset. So, we chose to standardize data before applying PCA.\n",
    "\n",
    "\n",
    "***Multi-class Classification Algorithms:***\n",
    "1. Random Forest\n",
    "2. Decision Trees\n",
    "3. K Nearest Neighbours\n",
    "\n",
    "Again, for comapring different classification algorithms, we took the best performing models from each of the 3 algorithms.\n",
    "\n",
    "The time taken to train the multi-class classification models is relatively lower than the binary classification models. This could be due to the fact that the train data size is smaller than the binary classification train data.\n",
    "\n",
    "As shown above in the analysis section, the dataset is highly imbalanced. In order to keep the data balanced over all classes, we first took most number of samples from the minority classes and sufficient number of sample from the majority clasees. Later we used SMOTE (Synthetic Minority Over-sampling Technique) to create an overall balanced dataset for training the multi-class classification models.\n",
    "\n",
    "By comparing the performance metrics of the models, we see that Random Forest is the best performing model followed by KNN and Decision Tree. From the confusion matrix and the classification report, the dominance of Random Forest in terms of precision, recall, and f1-score is evident. The reason Decision Tree falling behind is that it is not always expressive enough to capture complex relationships between the input features and the target variable. Decision Tree may struggle with problems where the target variable depends on a combination of input features rather than just one or two features. KNN and Random Forest can handle more complex relationships between the input features and the target variable by using more flexible models. Also, we used relatively fewer parameters to tune the Decision Tree models. Just like the binary classification algorithms, we also cross-validated the multi-class classification models to make sure they were not overfitted.\n",
    "\n",
    "\n",
    "**Future Work:** Keeping this in mind, we have to choose our model accordingly if we want to stick with one. However, In this case, we can also combine the KNN and Random Forest classifiers using an ensemble method. This can improve the accuracy of our intrusion detection system by leveraging the strengths of both models and reducing the risk of overfitting. The ensemble method would allow the models to work together to produce a more robust prediction, which could be more effective at identifying different types of network attack.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B8DGk4gEhiJp",
    "aSAAWG06ekQn",
    "LDdJgFihgVf8",
    "R28LNrp5iWR7",
    "xf9z8Pwlid-3",
    "hXWOYP6-ieI0",
    "MsSwQ-DOvm-6",
    "ZGkxFBOYv5Gt",
    "-uDiDKokwPhS",
    "_iYfyW2-yyAR",
    "NJ9tbZTRxHVU",
    "nNRw8GcEQ-st",
    "kk2Kas03g1V3",
    "R_y73RFP274P",
    "1tgkPDHR59TJ",
    "Db-kcdZVVrYX",
    "VHpB4fGaTzPK",
    "0bYBt91SVl2Q",
    "Bvab0SkCfy0d",
    "isyblWcu-wmg",
    "8D1ip-J88_fE",
    "0qr2iEpQYjmr",
    "tEDT9-gO6UfJ",
    "QUzFJzuL7K35",
    "O_5dqWNjXuRL",
    "21Q9ths2VJC-",
    "pDp-xQWSqtLV",
    "GIa0LHYKT78a",
    "GCVzGYzwXivR",
    "BxUFSY5JapE5"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "5df4995f4205929e0e7a5839a3f9bdb0ead66b703c002bad5d5fcff442b16b20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
